networks:
  linqra-network:
    external: true
    driver_opts:
      com.docker.network.driver.mtu: 1500

services:
  # postgres
  postgres-service:
    build:
      context: .
      dockerfile: ./.kube/postgres/Dockerfile
    command: postgres -c "max_connections=200"
    restart: always
    ports:
      - "5432:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}" ]
      interval: 10s
      timeout: 3s
      retries: 3
    volumes:
      - ./.kube/postgres/data/:/var/lib/postgresql/pgdata
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    networks:
      - linqra-network


  # postgres admin
  pgadmin-service:
    build:
      context: .
      dockerfile: ./.kube/pgadmin/Dockerfile
    restart: always
    ports:
      - "9090:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: user-name@domain-name.com
      PGADMIN_DEFAULT_PASSWORD: strong-password
    volumes:
      - ./.kube/pgadmin/data/:/var/lib/pgadmin
    networks:
      - linqra-network

  # keycloak
  keycloak-service:
    build:
      context: .
      dockerfile: ./.kube/keycloak/Dockerfile
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres-service:5432/${POSTGRES_DB}
      KC_DB_USERNAME: ${POSTGRES_USER}
      KC_DB_PASSWORD: ${POSTGRES_PASSWORD}

      KC_HTTP_ENABLED: true
      KC_HTTP_PORT: 8080
      KC_HTTP_RELATIVE_PATH: /keycloak

      KC_HOSTNAME: https://linqra.com
      KC_HOSTNAME_STRICT: true
      KC_HOSTNAME_STRICT_HTTPS: true
      KC_HOSTNAME_ADMIN: https://linqra.com/keycloak
      KC_HOSTNAME_URL: https://linqra.com/keycloak
      KC_FRONTEND_URL: https://linqra.com/keycloak

      KC_PROXY: edge
      PROXY_ADDRESS_FORWARDING: true

      KC_LOG_LEVEL: info
      KC_METRICS_ENABLED: true
      KC_HEALTH_ENABLED: true
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
      KC_FEATURES: admin

      KC_SPI_RESOURCES_PUBLIC_URL: https://linqra.com/keycloak/resources
      KC_SPI_LOGIN_PROTOCOL_OPENID_CONNECT_REDIRECT_HOSTNAME: https://linqra.com/keycloak
      KC_SPI_LOGIN_PROTOCOL_OPENID_CONNECT_LEGACY_IFRAME_COMPATIBLE: "true"
      KC_SPI_LOGIN_PROTOCOL_OPENID_CONNECT_SUPPRESS_LOGOUT_CONFIRMATION_SCREEN: "true"
    command:
      - start
    volumes:
      - ./.kube/keycloak/data/import:/opt/keycloak/data/import
      - ./.kube/keycloak/data/export:/opt/keycloak/data/export
      - ./.kube/keycloak/themes:/opt/keycloak/themes
    depends_on:
      postgres-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "-H", "Host: localhost", "http://localhost:8080/health/ready"]
      interval: 10s
      timeout: 3s
      retries: 3
    ports:
      - "8281:8080"
    networks:
      - linqra-network

  # mongodb nodes
  mongodb1:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    build:
      context: .
      dockerfile: ./.kube/mongodb/Dockerfile
    container_name: mongodb1
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_REPLICA_SET_NAME: rs0
    ports:
      - "27017:27017"
    volumes:
      - ./.kube/mongodb/data1/:/data/db
      - ./.kube/mongodb/mongo-keyfile:/data/mongo-keyfile
    networks:
      - linqra-network
    command: mongod --bind_ip_all --replSet rs0 --port 27017 -keyFile /data/mongo-keyfile

  mongodb2:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    build:
      context: .
      dockerfile: ./.kube/mongodb/Dockerfile
    container_name: mongodb2
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_REPLICA_SET_NAME: rs0
    ports:
      - "27018:27018"
    volumes:
      - ./.kube/mongodb/data2/:/data/db
      - ./.kube/mongodb/mongo-keyfile:/data/mongo-keyfile
    networks:
      - linqra-network
    command: mongod  --bind_ip_all  --replSet rs0 --port 27018 -keyFile /data/mongo-keyfile

  mongodb3:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    build:
      context: .
      dockerfile: ./.kube/mongodb/Dockerfile
    container_name: mongodb3
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
      MONGO_REPLICA_SET_NAME: rs0
    ports:
      - "27019:27019"
    volumes:
      - ./.kube/mongodb/data3/:/data/db
      - ./.kube/mongodb/mongo-keyfile:/data/mongo-keyfile
    networks:
      - linqra-network
    command: mongod --bind_ip_all --replSet rs0 --port 27019 -keyFile /data/mongo-keyfile


  # redis
  redis-service:
    build:
      context: .
      dockerfile: ./.kube/redis/Dockerfile
    environment:
      REDIS_GATEWAY_URL: redis-service
    ports:
      - "6379:6379"
    volumes:
      - ./.kube/redis/data/:/var/lib/redis/data
      - ./.kube/redis/redis.conf:/usr/local/etc/redis/redis.conf
    command: ["redis-server", "/usr/local/etc/redis/redis.conf"]
    networks:
      - linqra-network

  # etcd for Milvus
  etcd-service:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ./.kube/etcd/data:/etcd
    command: etcd -advertise-client-urls=http://etcd-service:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - linqra-network

  # MinIO for Milvus
  minio-service:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      # MinIO credentials stored in vault - passed via environment variables
      # These should be set externally: MINIO_ACCESS_KEY, MINIO_SECRET_KEY
      # Or stored in vault and injected by a script/init container
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
    volumes:
      - ./.kube/minio/data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - linqra-network

  # Milvus vector database
  milvus-service:
    build:
      context: .
      dockerfile: ./.kube/milvus/Dockerfile
    environment:
      ETCD_ENDPOINTS: etcd-service:2379
      MINIO_ADDRESS: minio-service:9000
      # MinIO credentials stored in vault - passed via environment variables
      # These should be set externally: MINIO_ACCESS_KEY, MINIO_SECRET_KEY
      # Or stored in vault and injected by a script/init container
      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
      COMMON_CFG_RETENTION_DURATION: 168h
      MILVUS_AUTH_ENABLED: "true"
      # Milvus credentials stored in vault - passed via environment variables
      # These should match vault.milvus.username and vault.milvus.password
      MILVUS_USERNAME: ${MILVUS_USERNAME}
      MILVUS_PASSWORD: ${MILVUS_PASSWORD}
    volumes:
      - ./.kube/milvus/data:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    networks:
      - linqra-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
      restart_policy:
        condition: on-failure
    depends_on:
      - etcd-service
      - minio-service
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/api/v1/health"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Attu UI for Milvus
  attu-service:
    build:
      context: .
      dockerfile: ./.kube/attu/Dockerfile
    environment:
      MILVUS_URL: milvus-service:19530
      MILVUS_DATABASE: default
      MILVUS_USE_SSL: "false"
      MILVUS_INSECURE: "true"
      MILVUS_AUTH_ENABLED: "true"
      MILVUS_AUTH_USERNAME: ${MILVUS_USERNAME}
      MILVUS_AUTH_PASSWORD: ${MILVUS_PASSWORD}
    ports:
      - "8000:3000"
    networks:
      - linqra-network
    depends_on:
      - milvus-service

  # Neo4j Graph Database
  neo4j-service:
    build:
      context: .
      dockerfile: ./.kube/neo4j/Dockerfile
    container_name: neo4j-service
    environment:
      - NEO4J_AUTH=${NEO4J_USERNAME}/${NEO4J_PASSWORD}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_memory_pagecache_size=1G
      - NEO4J_dbms_connector_bolt_listen__address=0.0.0.0:7687
      - NEO4J_dbms_connector_http_listen__address=0.0.0.0:7474
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*
      # Enable APOC procedures for advanced graph operations
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt (Java driver uses this)
    volumes:
      - ./.kube/neo4j/data:/data
      - ./.kube/neo4j/logs:/logs
      - ./.kube/neo4j/import:/var/lib/neo4j/import
      - ./.kube/neo4j/plugins:/plugins
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:7474/"]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - linqra-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 3G
        reservations:
          cpus: '0.5'
          memory: 1G
      restart_policy:
        condition: on-failure

  ##### ENABLE THIS FOR DOCKERIZATION FOR ALL REMAINING SERVICES
  # Discovery Server (Eureka)
  discovery-service:
    build:
      context: .
      dockerfile: .kube/eureka/Dockerfile
    #    image: discovery-service:latest
    environment:
      # Vault configuration - discovery-server reads secrets from vault via VaultPropertySource (like api-gateway)
      # All Eureka SSL configuration (keystore, truststore, passwords) are stored in vault and read via ${vault.*} references in application.yml
      # Vault file is mounted at /app/secrets/vault.encrypted (see volumes section)
      # VAULT_ENVIRONMENT not set - will use SPRING_PROFILES_ACTIVE (ec2) to map to "ec2" environment
      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
    ports:
      - "8761:8761"
    volumes:
      - ./discovery-server:/app/discovery-server
      - ./keys:/app/keys
      - ./secrets:/app/secrets:ro  # Mount vault file (read-only for discovery-server)
    networks:
      - linqra-network


  # Load Balancer
  gateway-loadbalancer:
    image: haproxy:2.8
    ports:
      - "7777:7777"
    volumes:
      - ./.kube/gateway/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - ./keys/haproxy/gateway-combined-container.pem:/etc/ssl/gateway-cert.pem:ro
    networks:
      - linqra-network
    depends_on:
      - api-gateway-service

  # API Gateway Service
  api-gateway-service:
    build:
      context: .
      dockerfile: ./.kube/gateway/Dockerfile
    environment:
      SPRING_PROFILES_ACTIVE: ec2
      SPRING_REDIS_TIMEOUT: 5000
      SPRING_REDIS_CONNECT_TIMEOUT: 5000
      SPRING_REDIS_POOL_MAX_ACTIVE: 10
      SPRING_REDIS_POOL_MAX_IDLE: 5
      SPRING_REDIS_POOL_MIN_IDLE: 2
      SPRING_REDIS_POOL_MAX_WAIT: 5000

      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}

      # Docker networking optimizations
      JAVA_OPTS: "-Djava.net.preferIPv4Stack=true -Dsun.net.client.defaultConnectTimeout=5000 -Dsun.net.client.defaultReadTimeout=30000"
      DOCKER_CONTAINER: "true"
      # Debug logging for container identification
      #LOGGING_LEVEL_ORG_LITE_GATEWAY: "DEBUG"
      #LOGGING_LEVEL_REACTOR_NETTY: "DEBUG"
      #LOGGING_LEVEL_ORG_SPRING_DATA_MONGODB: "DEBUG"
      #LOGGING_LEVEL_ORG_MONGODB_DRIVER: "DEBUG"
    volumes:
      - ./api-gateway:/app/api-gateway
      - ./keys:/app/keys
      - ./secrets:/app/secrets:rw  # Vault file directory (read-write) - PERSISTS on host across container restarts
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
      replicas: 1
      restart_policy:
        condition: on-failure
    expose:
      - "7777"
    networks:
      - linqra-network
    depends_on:
      - discovery-service
      - mongodb1
      - mongodb2
      - mongodb3
      - keycloak-service
      - redis-service
      - neo4j-service