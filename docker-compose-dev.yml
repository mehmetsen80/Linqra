networks:
  linqra-network:
    external: true

services:
  # postgres
  postgres-service:
    build:
      context: .
      dockerfile: ./.kube/postgres/Dockerfile
    container_name: postgres-service
    command: postgres -c "max_connections=200"
    restart: always
    ports:
      - "5432:5432"
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U $POSTGRES_USER -d $POSTGRES_DB" ]
      interval: 10s
      timeout: 3s
      retries: 3
    volumes:
      - ./.kube/postgres/data/:/var/lib/postgresql/pgdata
      - ./secrets:/app/secrets:ro
    environment:
      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
      VAULT_ENVIRONMENT: ${VAULT_ENVIRONMENT:-dev}
      VAULT_REQUIRED_VARS: POSTGRES_DB,POSTGRES_USER,POSTGRES_PASSWORD
    networks:
      - linqra-network

  # postgres admin
  pgadmin-service:
    build:
      context: .
      dockerfile: ./.kube/pgadmin/Dockerfile
    restart: always
    ports:
      - "9090:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: user-name@domain-name.com
      PGADMIN_DEFAULT_PASSWORD: strong-password
    volumes:
      - ./.kube/pgadmin/data/:/var/lib/pgadmin
    networks:
      - linqra-network

  # keycloak
  keycloak-service:
    build:
      context: .
      dockerfile: ./.kube/keycloak/Dockerfile
    environment:
      KC_DB: postgres
      KC_HTTP_ENABLED: true
      KC_HTTP_PORT: 8080
      KC_HOSTNAME: localhost
      KC_HOSTNAME_PORT: 8281
      KC_HOSTNAME_STRICT: false
      KC_HOSTNAME_STRICT_HTTPS: false
      KC_HOSTNAME_STRICT_BACKCHANNEL: false
      KC_HOSTNAME_URL: http://localhost:8281

      KC_LOG_LEVEL: info
      KC_METRICS_ENABLED: true
      KC_HEALTH_ENABLED: true
      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
      VAULT_ENVIRONMENT: ${VAULT_ENVIRONMENT:-dev}
      VAULT_REQUIRED_VARS: POSTGRES_DB,POSTGRES_USER,POSTGRES_PASSWORD,KEYCLOAK_ADMIN,KEYCLOAK_ADMIN_PASSWORD

    command:
      - start-dev
      - --import-realm
    volumes:
      - ./.kube/keycloak/data/import:/opt/keycloak/data/import
      - ./.kube/keycloak/data/export:/opt/keycloak/data/export
      - ./.kube/keycloak/themes:/opt/keycloak/themes
      - ./secrets:/app/secrets:ro
    depends_on:
      postgres-service:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "exec 3<>/dev/tcp/127.0.0.1/8080;echo -e 'GET /health/ready HTTP/1.1\r

            Host: localhost\r

            Connection: close\r

            \r

            ' >&3;if [ $? -eq 0 ]; then echo 'Healthcheck Successful';exit 0;else echo 'Healthcheck Failed';exit 1;fi;"
        ]
      interval: 10s
      timeout: 3s
      retries: 3
    ports:
      - "8281:8080"
    networks:
      - linqra-network

  # mongodb nodes
  mongodb1:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    build:
      context: .
      dockerfile: ./.kube/mongodb/Dockerfile
    container_name: mongodb1
    environment:
      MONGO_REPLICA_SET_NAME: rs0
      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
      VAULT_ENVIRONMENT: ${VAULT_ENVIRONMENT:-dev}
      VAULT_REQUIRED_VARS: MONGO_INITDB_ROOT_USERNAME,MONGO_INITDB_ROOT_PASSWORD
    ports:
      - "27017:27017"
    volumes:
      - ./.kube/mongodb/data1/:/data/db
      - ./.kube/mongodb/mongo-keyfile:/data/mongo-keyfile
      - ./secrets:/app/secrets:ro
    networks:
      - linqra-network
    command: mongod --bind_ip_all --replSet rs0 --port 27017 -keyFile /data/mongo-keyfile

  mongodb2:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    build:
      context: .
      dockerfile: ./.kube/mongodb/Dockerfile
    container_name: mongodb2
    environment:
      MONGO_REPLICA_SET_NAME: rs0
      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
      VAULT_ENVIRONMENT: ${VAULT_ENVIRONMENT:-dev}
      VAULT_REQUIRED_VARS: MONGO_INITDB_ROOT_USERNAME,MONGO_INITDB_ROOT_PASSWORD
    ports:
      - "27018:27018"
    volumes:
      - ./.kube/mongodb/data2/:/data/db
      - ./.kube/mongodb/mongo-keyfile:/data/mongo-keyfile
      - ./secrets:/app/secrets:ro
    networks:
      - linqra-network
    command: mongod  --bind_ip_all  --replSet rs0 --port 27018 -keyFile /data/mongo-keyfile

  mongodb3:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    build:
      context: .
      dockerfile: ./.kube/mongodb/Dockerfile
    container_name: mongodb3
    environment:
      MONGO_REPLICA_SET_NAME: rs0
      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
      VAULT_ENVIRONMENT: ${VAULT_ENVIRONMENT:-dev}
      VAULT_REQUIRED_VARS: MONGO_INITDB_ROOT_USERNAME,MONGO_INITDB_ROOT_PASSWORD
    ports:
      - "27019:27019"
    volumes:
      - ./.kube/mongodb/data3/:/data/db
      - ./.kube/mongodb/mongo-keyfile:/data/mongo-keyfile
      - ./secrets:/app/secrets:ro
    networks:
      - linqra-network
    command: mongod --bind_ip_all --replSet rs0 --port 27019 -keyFile /data/mongo-keyfile

  # redis
  redis-service:
    build:
      context: .
      dockerfile: ./.kube/redis/Dockerfile
    environment:
      REDIS_GATEWAY_URL: redis-service
    ports:
      - "6379:6379"
    volumes:
      - ./.kube/redis/data/:/var/lib/redis/data
      - ./.kube/redis/redis.conf:/usr/local/etc/redis/redis.conf
    command: [ "redis-server", "/usr/local/etc/redis/redis.conf" ]
    networks:
      - linqra-network

  # etcd for Milvus
  etcd-service:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ./.kube/etcd/data:/etcd
    command: etcd -advertise-client-urls=http://etcd-service:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: [ "CMD", "etcdctl", "endpoint", "health" ]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - linqra-network

  # MinIO for Milvus
  minio-service:
    build:
      context: .
      dockerfile: ./.kube/minio/Dockerfile
    environment:
      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
      VAULT_ENVIRONMENT: ${VAULT_ENVIRONMENT:-dev}
      VAULT_REQUIRED_VARS: MINIO_ACCESS_KEY,MINIO_SECRET_KEY
    volumes:
      - ./.kube/minio/data:/data
      - ./secrets:/app/secrets:ro
    command: [ "server", "/data", "--console-address", ":9001" ]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - linqra-network

  # Milvus vector database
  milvus-service:
    build:
      context: .
      dockerfile: ./.kube/milvus/Dockerfile
    container_name: milvus-service
    environment:
      ETCD_ENDPOINTS: etcd-service:2379
      MINIO_ADDRESS: minio-service:9000
      MINIO_ACCESS_KEY: $MINIO_ACCESS_KEY
      MINIO_SECRET_KEY: $MINIO_SECRET_KEY
      COMMON_CFG_RETENTION_DURATION: 168h
      # Milvus authentication (correct env var for Milvus 2.x)
      COMMON_SECURITY_AUTHORIZATIONENABLED: "true"
      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
      VAULT_ENVIRONMENT: ${VAULT_ENVIRONMENT:-dev}
      VAULT_REQUIRED_VARS: MILVUS_USERNAME,MILVUS_PASSWORD,MINIO_ACCESS_KEY,MINIO_SECRET_KEY
    volumes:
      - ./.kube/milvus/data:/var/lib/milvus
      - ./secrets:/app/secrets:ro
    ports:
      - "19530:19530"
      - "9091:9091"
    networks:
      - linqra-network
    depends_on:
      - etcd-service
      - minio-service
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9091/api/v1/health" ]
      interval: 10s
      timeout: 3s
      retries: 3

  # Attu UI for Milvus
  attu-service:
    build:
      context: .
      dockerfile: ./.kube/attu/Dockerfile
    environment:
      MILVUS_URL: milvus-service:19530
      MILVUS_DATABASE: default
      MILVUS_USE_SSL: "false"
    ports:
      - "8000:3000"
    networks:
      - linqra-network
    depends_on:
      - milvus-service

  # Neo4j Graph Database
  neo4j-service:
    build:
      context: .
      dockerfile: ./.kube/neo4j/Dockerfile
    container_name: neo4j-service
    environment:
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_memory_pagecache_size=1G
      - NEO4J_dbms_connector_bolt_listen__address=0.0.0.0:7687
      - NEO4J_dbms_connector_http_listen__address=0.0.0.0:7474
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*
      - VAULT_MASTER_KEY=${VAULT_MASTER_KEY}
      - VAULT_ENVIRONMENT=${VAULT_ENVIRONMENT:-dev}
      - VAULT_REQUIRED_VARS=NEO4J_USERNAME,NEO4J_PASSWORD
    ports:
      - "7474:7474" # HTTP
      - "7687:7687" # Bolt (Java driver uses this)
    volumes:
      - ./.kube/neo4j/data:/data
      - ./.kube/neo4j/logs:/logs
      - ./.kube/neo4j/import:/var/lib/neo4j/import
      - ./.kube/neo4j/plugins:/plugins
      - ./secrets:/app/secrets:ro
    healthcheck:
      test: [ "CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:7474/" ]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - linqra-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 3G
        reservations:
          cpus: '0.5'
          memory: 1G
      restart_policy:
        condition: on-failure

##### ENABLE THIS FOR DOCKERIZATION FOR ALL REMAINING SERVICES IF YOU WANT TO RUN ALL SERVICES IN DOCKER
#  # Discovery Server (Eureka)
#   discovery-service:
#    build:
#      context: .
#      dockerfile: .kube/eureka/Dockerfile
#    environment:
#      # Vault configuration for discovery-server (needed to read vault secrets)
#      VAULT_ENVIRONMENT: dev-docker
#      # Vault file path is hardcoded to /app/secrets/vault.encrypted in application.yml
#      VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
#    ports:
#      - "8761:8761"
#    volumes:
#      - ./discovery-server:/app/discovery-server
#      - ./keys:/app/keys
#      - ./secrets:/app/secrets:ro  # Vault file directory (read-only for security)
#    networks:
#      - linqra-network


#  # Load Balancer
#   gateway-loadbalancer:
#    image: haproxy:2.8
#    ports:
#      - "7777:7777"
#    volumes:
#      - ./.kube/gateway/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
#      - ./keys/haproxy/gateway-combined-container.pem:/etc/ssl/gateway-cert.pem:ro
#    networks:
#      - linqra-network
#    depends_on:
#      - api-gateway-service

#   # API Gateway Service
#   api-gateway-service:
#     build:
#       context: .
#       dockerfile: ./.kube/gateway/Dockerfile
#     environment:
#       VAULT_ENVIRONMENT: dev-docker
#       VAULT_MASTER_KEY: ${VAULT_MASTER_KEY}
#       VAULT_REQUIRED_VARS: SPRING_PROFILES_ACTIVE,AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,GATEWAY_TRUSTSTORE_PASSWORD
#       # Redis connection pool optimizations for Docker
#       SPRING_REDIS_TIMEOUT: 5000
#       SPRING_REDIS_CONNECT_TIMEOUT: 5000
#       SPRING_REDIS_POOL_MAX_ACTIVE: 10
#       SPRING_REDIS_POOL_MAX_IDLE: 5
#       SPRING_REDIS_POOL_MIN_IDLE: 2
#       SPRING_REDIS_POOL_MAX_WAIT: 5000
#     volumes:
#       - ./keys:/app/keys
#       - ./secrets:/app/secrets:ro  # Vault file directory (read-only for security)
#     deploy:
#       replicas: 1
#       restart_policy:
#         condition: on-failure
#     expose:
#       - "7777"
#     networks:
#       - linqra-network
#     depends_on:
#       - discovery-service
#       - mongodb1
#       - mongodb2
#       - mongodb3
#       - keycloak-service
#       - redis-service
#       - neo4j-service
